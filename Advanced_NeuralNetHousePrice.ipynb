{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load the dataset\n",
    "# -----------------------------\n",
    "# Replace 'house_prices.csv' with the path to your dataset.\n",
    "data = pd.read_csv('train.csv')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Data Cleaning\n",
    "# -----------------------------\n",
    "# Select only numeric columns that have no missing data.\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "clean_numeric_cols = [col for col in numeric_cols if data[col].isna().sum() == 0]\n",
    "data_clean = data[clean_numeric_cols]\n",
    "\n",
    "# Ensure that the target column 'price' is present.\n",
    "if 'SalePrice' not in data_clean.columns:\n",
    "    raise ValueError(\"The target column 'price' is not present in the complete numeric data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 4 features: ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Feature Selection\n",
    "# -----------------------------\n",
    "# Compute the correlation matrix using only the cleaned numeric data.\n",
    "corr_matrix = data_clean.corr()\n",
    "\n",
    "# Compute absolute correlations of features with the target and drop the target itself.\n",
    "target_corr = corr_matrix['SalePrice'].drop('SalePrice').abs().sort_values(ascending=False)\n",
    "\n",
    "# Select only the top 4 features with the highest correlation with 'SalesPrice'\n",
    "top4_features = target_corr.head(4).index\n",
    "print(\"Selected top 4 features:\", list(top4_features))\n",
    "\n",
    "# Define input features (X) and target variable (y).\n",
    "X = data_clean[top4_features].values\n",
    "y = data_clean['SalePrice'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Data Preprocessing\n",
    "# -----------------------------\n",
    "# Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features to improve training stability.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to PyTorch tensors.\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader for batch processing.\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class HousePriceModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HousePriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output layer for regression\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.1060, LR: 0.000900\n",
      "Epoch [20/1000], Loss: 0.0834, LR: 0.000810\n",
      "Epoch [30/1000], Loss: 0.0985, LR: 0.000729\n",
      "Epoch [40/1000], Loss: 0.1000, LR: 0.000656\n",
      "Epoch [50/1000], Loss: 0.0666, LR: 0.000590\n",
      "Epoch [60/1000], Loss: 0.0793, LR: 0.000531\n",
      "Epoch [70/1000], Loss: 0.0663, LR: 0.000478\n",
      "Epoch [80/1000], Loss: 0.0788, LR: 0.000430\n",
      "Epoch [90/1000], Loss: 0.0755, LR: 0.000387\n",
      "Epoch [100/1000], Loss: 0.0551, LR: 0.000349\n",
      "Epoch [110/1000], Loss: 0.0641, LR: 0.000314\n",
      "Epoch [120/1000], Loss: 0.0593, LR: 0.000282\n",
      "Epoch [130/1000], Loss: 0.0570, LR: 0.000254\n",
      "Epoch [140/1000], Loss: 0.0616, LR: 0.000229\n",
      "Epoch [150/1000], Loss: 0.0564, LR: 0.000206\n",
      "Epoch [160/1000], Loss: 0.0546, LR: 0.000185\n",
      "Epoch [170/1000], Loss: 0.0561, LR: 0.000167\n",
      "Epoch [180/1000], Loss: 0.0567, LR: 0.000150\n",
      "Epoch [190/1000], Loss: 0.0625, LR: 0.000135\n",
      "Epoch [200/1000], Loss: 0.0522, LR: 0.000122\n",
      "Epoch [210/1000], Loss: 0.0530, LR: 0.000109\n",
      "Epoch [220/1000], Loss: 0.0549, LR: 0.000098\n",
      "Epoch [230/1000], Loss: 0.0551, LR: 0.000089\n",
      "Epoch [240/1000], Loss: 0.0511, LR: 0.000080\n",
      "Epoch [250/1000], Loss: 0.0517, LR: 0.000072\n",
      "Epoch [260/1000], Loss: 0.0575, LR: 0.000065\n",
      "Epoch [270/1000], Loss: 0.0589, LR: 0.000058\n",
      "Epoch [280/1000], Loss: 0.0556, LR: 0.000052\n",
      "Epoch [290/1000], Loss: 0.0461, LR: 0.000047\n",
      "Epoch [300/1000], Loss: 0.0461, LR: 0.000042\n",
      "Epoch [310/1000], Loss: 0.0500, LR: 0.000038\n",
      "Epoch [320/1000], Loss: 0.0582, LR: 0.000034\n",
      "Epoch [330/1000], Loss: 0.0486, LR: 0.000031\n",
      "Epoch [340/1000], Loss: 0.0551, LR: 0.000028\n",
      "Epoch [350/1000], Loss: 0.0537, LR: 0.000025\n",
      "Epoch [360/1000], Loss: 0.0550, LR: 0.000023\n",
      "Epoch [370/1000], Loss: 0.0507, LR: 0.000020\n",
      "Epoch [380/1000], Loss: 0.0480, LR: 0.000018\n",
      "Epoch [390/1000], Loss: 0.0481, LR: 0.000016\n",
      "Epoch [400/1000], Loss: 0.0508, LR: 0.000015\n",
      "Epoch [410/1000], Loss: 0.0561, LR: 0.000013\n",
      "Epoch [420/1000], Loss: 0.0525, LR: 0.000012\n",
      "Epoch [430/1000], Loss: 0.0527, LR: 0.000011\n",
      "Epoch [440/1000], Loss: 0.0529, LR: 0.000010\n",
      "Epoch [450/1000], Loss: 0.0462, LR: 0.000009\n",
      "Epoch [460/1000], Loss: 0.0523, LR: 0.000008\n",
      "Epoch [470/1000], Loss: 0.0447, LR: 0.000007\n",
      "Epoch [480/1000], Loss: 0.0467, LR: 0.000006\n",
      "Epoch [490/1000], Loss: 0.0541, LR: 0.000006\n",
      "Epoch [500/1000], Loss: 0.0463, LR: 0.000005\n",
      "Epoch [510/1000], Loss: 0.0506, LR: 0.000005\n",
      "Epoch [520/1000], Loss: 0.0443, LR: 0.000004\n",
      "Epoch [530/1000], Loss: 0.0502, LR: 0.000004\n",
      "Epoch [540/1000], Loss: 0.0480, LR: 0.000003\n",
      "Epoch [550/1000], Loss: 0.0527, LR: 0.000003\n",
      "Epoch [560/1000], Loss: 0.0528, LR: 0.000003\n",
      "Epoch [570/1000], Loss: 0.0536, LR: 0.000002\n",
      "Epoch [580/1000], Loss: 0.0534, LR: 0.000002\n",
      "Epoch [590/1000], Loss: 0.0521, LR: 0.000002\n",
      "Epoch [600/1000], Loss: 0.0462, LR: 0.000002\n",
      "Epoch [610/1000], Loss: 0.0506, LR: 0.000002\n",
      "Epoch [620/1000], Loss: 0.0546, LR: 0.000001\n",
      "Epoch [630/1000], Loss: 0.0569, LR: 0.000001\n",
      "Epoch [640/1000], Loss: 0.0518, LR: 0.000001\n",
      "Epoch [650/1000], Loss: 0.0535, LR: 0.000001\n",
      "Epoch [660/1000], Loss: 0.0551, LR: 0.000001\n",
      "Epoch [670/1000], Loss: 0.0521, LR: 0.000001\n",
      "Epoch [680/1000], Loss: 0.0528, LR: 0.000001\n",
      "Epoch [690/1000], Loss: 0.0472, LR: 0.000001\n",
      "Epoch [700/1000], Loss: 0.0505, LR: 0.000001\n",
      "Epoch [710/1000], Loss: 0.0512, LR: 0.000001\n",
      "Epoch [720/1000], Loss: 0.0498, LR: 0.000001\n",
      "Epoch [730/1000], Loss: 0.0516, LR: 0.000000\n",
      "Epoch [740/1000], Loss: 0.0502, LR: 0.000000\n",
      "Epoch [750/1000], Loss: 0.0490, LR: 0.000000\n",
      "Epoch [760/1000], Loss: 0.0544, LR: 0.000000\n",
      "Epoch [770/1000], Loss: 0.0529, LR: 0.000000\n",
      "Epoch [780/1000], Loss: 0.0517, LR: 0.000000\n",
      "Epoch [790/1000], Loss: 0.0498, LR: 0.000000\n",
      "Epoch [800/1000], Loss: 0.0524, LR: 0.000000\n",
      "Epoch [810/1000], Loss: 0.0507, LR: 0.000000\n",
      "Epoch [820/1000], Loss: 0.0523, LR: 0.000000\n",
      "Epoch [830/1000], Loss: 0.0472, LR: 0.000000\n",
      "Epoch [840/1000], Loss: 0.0522, LR: 0.000000\n",
      "Epoch [850/1000], Loss: 0.0499, LR: 0.000000\n",
      "Epoch [860/1000], Loss: 0.0461, LR: 0.000000\n",
      "Epoch [870/1000], Loss: 0.0506, LR: 0.000000\n",
      "Epoch [880/1000], Loss: 0.0492, LR: 0.000000\n",
      "Epoch [890/1000], Loss: 0.0494, LR: 0.000000\n",
      "Epoch [900/1000], Loss: 0.0478, LR: 0.000000\n",
      "Epoch [910/1000], Loss: 0.0498, LR: 0.000000\n",
      "Epoch [920/1000], Loss: 0.0555, LR: 0.000000\n",
      "Epoch [930/1000], Loss: 0.0466, LR: 0.000000\n",
      "Epoch [940/1000], Loss: 0.0525, LR: 0.000000\n",
      "Epoch [950/1000], Loss: 0.0567, LR: 0.000000\n",
      "Epoch [960/1000], Loss: 0.0465, LR: 0.000000\n",
      "Epoch [970/1000], Loss: 0.0541, LR: 0.000000\n",
      "Epoch [980/1000], Loss: 0.0542, LR: 0.000000\n",
      "Epoch [990/1000], Loss: 0.0500, LR: 0.000000\n",
      "Epoch [1000/1000], Loss: 0.0496, LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Set Up Loss Function, Optimizer, and Learning Rate Scheduler\n",
    "# -----------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Scheduler: Decay LR by 10% every 10 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train the Model\n",
    "# -----------------------------\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    # Step the scheduler at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.06559764593839645\n",
      "Test MSE (scikit-learn): 0.06559767216454084\n"
     ]
    }
   ],
   "source": [
    "# 8. Evaluate the Model\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor.to(device))\n",
    "    test_loss = criterion(predictions, y_test_tensor.to(device)).item()\n",
    "    print(\"Test Mean Squared Error:\", test_loss)\n",
    "\n",
    "# Optionally, to evaluate using scikit-learn's MSE:\n",
    "predictions_np = predictions.cpu().numpy()\n",
    "mse = mean_squared_error(y_test, predictions_np)\n",
    "print(\"Test MSE (scikit-learn):\", mse)\n",
    "#Test Mean Squared Error: 935741376.0\n",
    "#Test Mean Squared Error: 890287168.0\n",
    "#Test MSE (scikit-learn): 890287040.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.19775629866294928\n",
      "Test RMSE: 0.2561203739228811\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predictions = model(X_train_tensor.to(device))\n",
    "    train_loss = criterion(train_predictions, y_train_tensor.to(device)).item()\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    print(\"Training RMSE:\", train_rmse)\n",
    "\n",
    "rmse = np.sqrt(test_loss)\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n",
    "#RESULT: Training RMSE: 28463.821247330794\n",
    "#Test RMSE: 29480.813014569325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9603/3042700335.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  errors = np.abs(predictions - y_test)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "errors = np.abs(predictions - y_test)\n",
    "sns.histplot(errors, bins=50, kde=True)\n",
    "plt.xlabel(\"Absolute Error ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Prediction Errors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Generate Predictions on New Test Data (test.csv)\n",
    "# -----------------------------\n",
    "# Load the new test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Ensure the test data contains the ID column and the same top4_features.\n",
    "# For this example, assume the test file contains an \"ID\" column.\n",
    "if 'Id' not in test_data.columns:\n",
    "    raise ValueError(\"The test data must contain an 'ID' column.\")\n",
    "\n",
    "# It's assumed that the test data has at least the columns used in training.\n",
    "# Use the same top4_features determined from the training set.\n",
    "X_new = test_data[top4_features].values\n",
    "\n",
    "# Apply the same scaling transformation using the previously fitted scaler.\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Convert to PyTorch tensor.\n",
    "X_new_tensor = torch.tensor(X_new_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Put the model in evaluation mode and generate predictions.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_predictions = model(X_new_tensor)\n",
    "    \n",
    "# Convert predictions to numpy array and flatten if necessary.\n",
    "new_predictions_np = new_predictions.cpu().numpy().flatten()\n",
    "\n",
    "# Create a DataFrame with IDs and their corresponding predicted sale prices.\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': test_data['Id'],\n",
    "    'SalePrice': new_predictions_np\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file.\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions saved to predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 4 features: ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 12.9142\n",
      "Epoch [20/1000], Loss: 0.3353\n",
      "Epoch [30/1000], Loss: 0.3849\n",
      "Epoch [40/1000], Loss: 0.2801\n",
      "Epoch [50/1000], Loss: 0.2218\n",
      "Epoch [60/1000], Loss: 0.1865\n",
      "Epoch [70/1000], Loss: 0.1997\n",
      "Epoch [80/1000], Loss: 0.1942\n",
      "Epoch [90/1000], Loss: 0.1637\n",
      "Epoch [100/1000], Loss: 0.1652\n",
      "Epoch [110/1000], Loss: 0.1842\n",
      "Epoch [120/1000], Loss: 0.1266\n",
      "Epoch [130/1000], Loss: 0.1170\n",
      "Epoch [140/1000], Loss: 0.1347\n",
      "Epoch [150/1000], Loss: 0.1232\n",
      "Epoch [160/1000], Loss: 0.1275\n",
      "Epoch [170/1000], Loss: 0.1304\n",
      "Epoch [180/1000], Loss: 0.1215\n",
      "Epoch [190/1000], Loss: 0.1184\n",
      "Epoch [200/1000], Loss: 0.0961\n",
      "Epoch [210/1000], Loss: 0.1036\n",
      "Epoch [220/1000], Loss: 0.0890\n",
      "Epoch [230/1000], Loss: 0.0832\n",
      "Epoch [240/1000], Loss: 0.0841\n",
      "Epoch [250/1000], Loss: 0.0952\n",
      "Epoch [260/1000], Loss: 0.0752\n",
      "Epoch [270/1000], Loss: 0.0848\n",
      "Epoch [280/1000], Loss: 0.0717\n",
      "Epoch [290/1000], Loss: 0.0806\n",
      "Epoch [300/1000], Loss: 0.0749\n",
      "Epoch [310/1000], Loss: 0.0814\n",
      "Epoch [320/1000], Loss: 0.0852\n",
      "Epoch [330/1000], Loss: 0.0862\n",
      "Epoch [340/1000], Loss: 0.0809\n",
      "Epoch [350/1000], Loss: 0.0769\n",
      "Epoch [360/1000], Loss: 0.0892\n",
      "Epoch [370/1000], Loss: 0.0956\n",
      "Epoch [380/1000], Loss: 0.0888\n",
      "Epoch [390/1000], Loss: 0.0931\n",
      "Epoch [400/1000], Loss: 0.0889\n",
      "Epoch [410/1000], Loss: 0.0874\n",
      "Epoch [420/1000], Loss: 0.0753\n",
      "Epoch [430/1000], Loss: 0.0821\n",
      "Epoch [440/1000], Loss: 0.0865\n",
      "Epoch [450/1000], Loss: 0.0733\n",
      "Epoch [460/1000], Loss: 0.0668\n",
      "Epoch [470/1000], Loss: 0.0767\n",
      "Epoch [480/1000], Loss: 0.0768\n",
      "Epoch [490/1000], Loss: 0.0801\n",
      "Epoch [500/1000], Loss: 0.0777\n",
      "Epoch [510/1000], Loss: 0.0850\n",
      "Epoch [520/1000], Loss: 0.0761\n",
      "Epoch [530/1000], Loss: 0.0859\n",
      "Epoch [540/1000], Loss: 0.0819\n",
      "Epoch [550/1000], Loss: 0.0816\n",
      "Epoch [560/1000], Loss: 0.0844\n",
      "Epoch [570/1000], Loss: 0.0947\n",
      "Epoch [580/1000], Loss: 0.0839\n",
      "Epoch [590/1000], Loss: 0.0893\n",
      "Epoch [600/1000], Loss: 0.0812\n",
      "Epoch [610/1000], Loss: 0.0816\n",
      "Epoch [620/1000], Loss: 0.0936\n",
      "Epoch [630/1000], Loss: 0.0878\n",
      "Epoch [640/1000], Loss: 0.0744\n",
      "Epoch [650/1000], Loss: 0.0811\n",
      "Epoch [660/1000], Loss: 0.0820\n",
      "Epoch [670/1000], Loss: 0.1003\n",
      "Epoch [680/1000], Loss: 0.0767\n",
      "Epoch [690/1000], Loss: 0.0789\n",
      "Epoch [700/1000], Loss: 0.0836\n",
      "Epoch [710/1000], Loss: 0.0764\n",
      "Epoch [720/1000], Loss: 0.0821\n",
      "Epoch [730/1000], Loss: 0.0761\n",
      "Epoch [740/1000], Loss: 0.0767\n",
      "Epoch [750/1000], Loss: 0.0908\n",
      "Epoch [760/1000], Loss: 0.0716\n",
      "Epoch [770/1000], Loss: 0.0834\n",
      "Epoch [780/1000], Loss: 0.0798\n",
      "Epoch [790/1000], Loss: 0.0657\n",
      "Epoch [800/1000], Loss: 0.0930\n",
      "Epoch [810/1000], Loss: 0.0771\n",
      "Epoch [820/1000], Loss: 0.0879\n",
      "Epoch [830/1000], Loss: 0.0813\n",
      "Epoch [840/1000], Loss: 0.0829\n",
      "Epoch [850/1000], Loss: 0.0822\n",
      "Epoch [860/1000], Loss: 0.0763\n",
      "Epoch [870/1000], Loss: 0.0768\n",
      "Epoch [880/1000], Loss: 0.0672\n",
      "Epoch [890/1000], Loss: 0.0855\n",
      "Epoch [900/1000], Loss: 0.0816\n",
      "Epoch [910/1000], Loss: 0.0781\n",
      "Epoch [920/1000], Loss: 0.0778\n",
      "Epoch [930/1000], Loss: 0.0689\n",
      "Epoch [940/1000], Loss: 0.0867\n",
      "Epoch [950/1000], Loss: 0.0753\n",
      "Epoch [960/1000], Loss: 0.0789\n",
      "Epoch [970/1000], Loss: 0.0919\n",
      "Epoch [980/1000], Loss: 0.0809\n",
      "Epoch [990/1000], Loss: 0.0688\n",
      "Epoch [1000/1000], Loss: 0.0807\n",
      "Test RMSE on original scale: 194698.0826619264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# -----------------------------\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Cleaning\n",
    "# -----------------------------\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "clean_numeric_cols = [col for col in numeric_cols if data[col].isna().sum() == 0]\n",
    "data_clean = data[clean_numeric_cols]\n",
    "\n",
    "if 'SalePrice' not in data_clean.columns:\n",
    "    raise ValueError(\"The target column 'SalePrice' is not present in the data.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Feature Selection and Target Transformation\n",
    "# -----------------------------\n",
    "# Compute correlation matrix and select top 4 features by absolute correlation with SalePrice.\n",
    "corr_matrix = data_clean.corr()\n",
    "target_corr = corr_matrix['SalePrice'].drop('SalePrice').abs().sort_values(ascending=False)\n",
    "top4_features = target_corr.head(4).index.tolist()\n",
    "print(\"Selected top 4 features:\", top4_features)\n",
    "\n",
    "X = data_clean[top4_features].values\n",
    "\n",
    "# Apply a log transformation to the target\n",
    "y = np.log(data_clean['SalePrice'].values.reshape(-1, 1))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Data Preprocessing\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Define a Simplified Neural Network Model\n",
    "# -----------------------------\n",
    "class LogPriceHousePriceModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogPriceHousePriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 1)  # Predicting log(SalePrice)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LogPriceHousePriceModel(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Set Up Loss Function and Optimizer\n",
    "# -----------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train the Model (1000 epochs)\n",
    "# -----------------------------\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluate the Model on Original Scale\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    log_predictions = model(X_test_tensor.to(device))\n",
    "    # Compute test loss on log scale\n",
    "    test_loss = criterion(log_predictions, y_test_tensor.to(device)).item()\n",
    "    \n",
    "    # Convert predictions back to original scale by exponentiating\n",
    "    predictions = torch.exp(log_predictions).cpu().numpy()\n",
    "    y_test_orig = np.exp(y_test)\n",
    "    \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_orig, predictions))\n",
    "    print(\"Test RMSE on original scale:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.21355211224633383\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_predictions = model(X_train_tensor.to(device))\n",
    "    train_loss = criterion(train_predictions, y_train_tensor.to(device)).item()\n",
    "    train_rmse = np.sqrt(train_loss)\n",
    "    print(\"Training RMSE:\", train_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 4 features: ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 11.1307\n",
      "Epoch [20/1000], Loss: 0.3696\n",
      "Epoch [30/1000], Loss: 0.3029\n",
      "Epoch [40/1000], Loss: 0.2911\n",
      "Epoch [50/1000], Loss: 0.2976\n",
      "Epoch [60/1000], Loss: 0.1913\n",
      "Epoch [70/1000], Loss: 0.1877\n",
      "Epoch [80/1000], Loss: 0.1688\n",
      "Epoch [90/1000], Loss: 0.1683\n",
      "Epoch [100/1000], Loss: 0.2043\n",
      "Epoch [110/1000], Loss: 0.1932\n",
      "Epoch [120/1000], Loss: 0.1565\n",
      "Epoch [130/1000], Loss: 0.1478\n",
      "Epoch [140/1000], Loss: 0.1651\n",
      "Epoch [150/1000], Loss: 0.1244\n",
      "Epoch [160/1000], Loss: 0.1562\n",
      "Epoch [170/1000], Loss: 0.1447\n",
      "Epoch [180/1000], Loss: 0.1085\n",
      "Epoch [190/1000], Loss: 0.1019\n",
      "Epoch [200/1000], Loss: 0.1667\n",
      "Epoch [210/1000], Loss: 0.0897\n",
      "Epoch [220/1000], Loss: 0.0869\n",
      "Epoch [230/1000], Loss: 0.1007\n",
      "Epoch [240/1000], Loss: 0.0821\n",
      "Epoch [250/1000], Loss: 0.0746\n",
      "Epoch [260/1000], Loss: 0.0755\n",
      "Epoch [270/1000], Loss: 0.0818\n",
      "Epoch [280/1000], Loss: 0.0795\n",
      "Epoch [290/1000], Loss: 0.0650\n",
      "Epoch [300/1000], Loss: 0.0679\n",
      "Epoch [310/1000], Loss: 0.0707\n",
      "Epoch [320/1000], Loss: 0.0756\n",
      "Epoch [330/1000], Loss: 0.0647\n",
      "Epoch [340/1000], Loss: 0.0687\n",
      "Epoch [350/1000], Loss: 0.0814\n",
      "Epoch [360/1000], Loss: 0.0815\n",
      "Epoch [370/1000], Loss: 0.0639\n",
      "Epoch [380/1000], Loss: 0.0779\n",
      "Epoch [390/1000], Loss: 0.0601\n",
      "Epoch [400/1000], Loss: 0.0539\n",
      "Epoch [410/1000], Loss: 0.0752\n",
      "Epoch [420/1000], Loss: 0.0827\n",
      "Epoch [430/1000], Loss: 0.0679\n",
      "Epoch [440/1000], Loss: 0.0640\n",
      "Epoch [450/1000], Loss: 0.0653\n",
      "Epoch [460/1000], Loss: 0.0788\n",
      "Epoch [470/1000], Loss: 0.0753\n",
      "Epoch [480/1000], Loss: 0.0596\n",
      "Epoch [490/1000], Loss: 0.0689\n",
      "Epoch [500/1000], Loss: 0.0807\n",
      "Epoch [510/1000], Loss: 0.0622\n",
      "Epoch [520/1000], Loss: 0.0600\n",
      "Epoch [530/1000], Loss: 0.0613\n",
      "Epoch [540/1000], Loss: 0.0592\n",
      "Epoch [550/1000], Loss: 0.0725\n",
      "Epoch [560/1000], Loss: 0.0661\n",
      "Epoch [570/1000], Loss: 0.0551\n",
      "Epoch [580/1000], Loss: 0.0571\n",
      "Epoch [590/1000], Loss: 0.0678\n",
      "Epoch [600/1000], Loss: 0.0650\n",
      "Epoch [610/1000], Loss: 0.0753\n",
      "Epoch [620/1000], Loss: 0.0692\n",
      "Epoch [630/1000], Loss: 0.0675\n",
      "Epoch [640/1000], Loss: 0.0665\n",
      "Epoch [650/1000], Loss: 0.0622\n",
      "Epoch [660/1000], Loss: 0.0700\n",
      "Epoch [670/1000], Loss: 0.0760\n",
      "Epoch [680/1000], Loss: 0.0740\n",
      "Epoch [690/1000], Loss: 0.0698\n",
      "Epoch [700/1000], Loss: 0.0569\n",
      "Epoch [710/1000], Loss: 0.0644\n",
      "Epoch [720/1000], Loss: 0.0689\n",
      "Epoch [730/1000], Loss: 0.0665\n",
      "Epoch [740/1000], Loss: 0.0768\n",
      "Epoch [750/1000], Loss: 0.0718\n",
      "Epoch [760/1000], Loss: 0.0716\n",
      "Epoch [770/1000], Loss: 0.0701\n",
      "Epoch [780/1000], Loss: 0.0743\n",
      "Epoch [790/1000], Loss: 0.0717\n",
      "Epoch [800/1000], Loss: 0.0683\n",
      "Epoch [810/1000], Loss: 0.0610\n",
      "Epoch [820/1000], Loss: 0.0733\n",
      "Epoch [830/1000], Loss: 0.0673\n",
      "Epoch [840/1000], Loss: 0.0629\n",
      "Epoch [850/1000], Loss: 0.0731\n",
      "Epoch [860/1000], Loss: 0.0572\n",
      "Epoch [870/1000], Loss: 0.0693\n",
      "Epoch [880/1000], Loss: 0.0667\n",
      "Epoch [890/1000], Loss: 0.0617\n",
      "Epoch [900/1000], Loss: 0.0703\n",
      "Epoch [910/1000], Loss: 0.0691\n",
      "Epoch [920/1000], Loss: 0.0817\n",
      "Epoch [930/1000], Loss: 0.0672\n",
      "Epoch [940/1000], Loss: 0.0738\n",
      "Epoch [950/1000], Loss: 0.0619\n",
      "Epoch [960/1000], Loss: 0.0602\n",
      "Epoch [970/1000], Loss: 0.0601\n",
      "Epoch [980/1000], Loss: 0.0640\n",
      "Epoch [990/1000], Loss: 0.0635\n",
      "Epoch [1000/1000], Loss: 0.0647\n",
      "Test RMSE on original scale: 75088.839263727\n",
      "Training RMSE on original scale: 42590.70088882653\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# -----------------------------\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Cleaning\n",
    "# -----------------------------\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "clean_numeric_cols = [col for col in numeric_cols if data[col].isna().sum() == 0]\n",
    "data_clean = data[clean_numeric_cols]\n",
    "\n",
    "if 'SalePrice' not in data_clean.columns:\n",
    "    raise ValueError(\"The target column 'SalePrice' is not present in the data.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Feature Selection and Target Transformation\n",
    "# -----------------------------\n",
    "# Compute correlation matrix and select top 4 features by absolute correlation with SalePrice.\n",
    "corr_matrix = data_clean.corr()\n",
    "target_corr = corr_matrix['SalePrice'].drop('SalePrice').abs().sort_values(ascending=False)\n",
    "top4_features = target_corr.head(4).index.tolist()\n",
    "print(\"Selected top 4 features:\", top4_features)\n",
    "\n",
    "X = data_clean[top4_features].values\n",
    "\n",
    "# Apply a log transformation to the target\n",
    "y = np.log(data_clean['SalePrice'].values.reshape(-1, 1))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Data Preprocessing\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Define a Simplified Neural Network Model\n",
    "# -----------------------------\n",
    "class LogPriceHousePriceModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogPriceHousePriceModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 1)  # Predicting log(SalePrice)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LogPriceHousePriceModel(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Set Up Loss Function, Optimizer, and Scheduler\n",
    "# -----------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Train the Model (1000 epochs)\n",
    "# -----------------------------\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Evaluate the Model on Original Scale\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Evaluate on Test Data\n",
    "    log_predictions_test = model(X_test_tensor.to(device))\n",
    "    test_loss = criterion(log_predictions_test, y_test_tensor.to(device)).item()\n",
    "    predictions_test = torch.exp(log_predictions_test).cpu().numpy()\n",
    "    y_test_orig = np.exp(y_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_orig, predictions_test))\n",
    "    print(\"Test RMSE on original scale:\", test_rmse)\n",
    "    \n",
    "    # Evaluate on Training Data\n",
    "    log_predictions_train = model(X_train_tensor.to(device))\n",
    "    train_loss = criterion(log_predictions_train, y_train_tensor.to(device)).item()\n",
    "    predictions_train = torch.exp(log_predictions_train).cpu().numpy()\n",
    "    y_train_orig = np.exp(y_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_orig, predictions_train))\n",
    "    print(\"Training RMSE on original scale:\", train_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
